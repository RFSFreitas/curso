---
title: "01-Ferramental"
author: "Julio Trecenti"
date: "July 28, 2016"
output: html_document
---

# Ferramental de trabalho da ABJ

As bases de dados utilizadas em estudos jurimétricos foram originalmente concebidas 
para fins gerenciais e não analíticos.
Por isso, observamos muitos dados faltantes, mal formatados e com
documentação inadequada.
Uma boa porção dos dados só está disponível em páginas HTML e arquivos PDF e grande parte da informação útil está escondida em textos.

Chamamos esse fenômeno de "pré-sal sociológico".
Temos hoje diversas bases de dados armazenadas em repositórios públicos ou 
controladas pelo poder público, mas que precisam ser lapidados para 
obtenção de informação útil.

Nessa parte do curso, vamos trabalhar com [*data wrangling*](http://r4ds.had.co.nz/wrangle-intro.html), 
que consiste em trabalhar com ferramentas de extração, 
consolidação e transformação de dados. 
As ferramentas utilizadas fazem parte do chamado `tidyverse`, 
um universo de pacotes contemporâneos do `R` que são intuitivas, eficientes e úteis.

Os pacotes utilizados são 
`httr`, `xml2`, `rvest`, 
`dplyr`, `tidyr`, `purrr`,
`lubridate`, `stringr`, 
`ggplot2`.
Também utilizamos um pacote chamado `abjutils`, 
construído para atender algumas necessidades frequentes na ABJ.
Recomenda-se a utilização do R 3.3.1 e o RStudio preview version.
É possível instalar todos esses pacotes de uma vez rodando

```{r eval=FALSE}
if (!require(devtools)) install.packages('devtools')
devtools::install_github('abjur/abjutils')
```

Recomenda-se também a utilização de `linux`.

```{r pacotes, echo=FALSE, message=FALSE, warning=FALSE}
require(dplyr)
require(tidyr)
data(pnud_muni, package = 'abjutils')
```

## Exemplos trabalhados

### Câmaras de gás

Uma das principais questões que surgem quando o tema é impunidade e que gerou as ideias iniciais desse artigo é: 
quando um réu condenado deve começar a cumprir pena? A justiça deve esperar o encerramento definitivo do
processo, com o chamado trânsito em julgado, ou pode iniciar o cumprimento já a partir de uma
decisão terminativa, como a sentença ou o acórdão de segundo grau?

Uma forma de solucionar esse debate é calcular as taxas de reforma de decisões
em matéria criminal. Uma condição necessária para a viabilidade da antecipação do
cumprimento de pena é uma baixa taxa de reforma das decisões, pois uma taxa alta implicaria que
muitas pessoas seriam presas injustamente. 

Com o objetivo de obter essas taxas, a presente pesquisa utiliza como base de dados um le-
vantamento de 157.379 decisões em segunda instância, das quais pouco menos de 60.000 envolvem
apelações contra o Ministério Público, todas proferidas entre 01/01/2014 e 31/12/2014 nas dezes-
seis Câmaras de Direito Criminal do Estado de São Paulo, e nas Câmaras Extraordinárias. Todas as
informações foram obtidas através de ferramentas computacionais a partir de bases de dados disponí-
veis publicamente, o que permite a reprodutibilidade da pesquisa. Os dados semi-estruturados foram
organizados a partir da utilização de técnicas de mineração de texto. Também foi necessário utilizar
procedimentos estatísticos adequados para lidar com problemas de dados faltantes.

Os resultados revelam taxas de reforma próximas a 50%. 
As taxas obtidas são relevantes e justificam a não antecipação
do cumprimento de pena para a decisão em primeira instância. Com o intuito de complementar e
aprofundar a pesquisa, realizamos análises para tipos específicos de crime, como roubo e tráfico de
drogas, comparando as taxas de reforma em cada subpopulação. Realizamos também a comparação
dos resultados relativamente às câmaras de julgamento e relatores.

A partir dessa análise, observamos uma alta variabilidade na taxa de reforma
entre as vinte câmaras de julgamento. 
Encontramos câmaras com mais de 75% de recursos negados
(quarta e sexta) e câmaras com menos de 30% de recursos negados (primeira, segunda e décima
segunda). O resultado é contraintuitivo pois teoricamente a alocação
de novos recursos nas câmaras é aleatória.

### Waze do Judiciário

A produtividade de varas e juízes é um tema corrente na administração do judiciário.
É importante mensurar a produtividade para fins de promoção e identificação de boas ou más práticas de atuação. No entanto, a complexidade processual, que não é observável, torna o problema de mensuração mais complicado do que simplesmente contar estatísticas de sentenças por mês.

Em 2014 a ABJ trabalhou com o TJSP na realização de um projeto para auxiliar na administração do judiciário.
Um dos objetivos desse projeto foi realizar análise de agrupamento de varas do Tribunal, com o intuito de detectar varas problemáticas e também varas-modelo, que poderiam auxiliar outras varas na gestão dos processos.

A análise de agrupamento pode ser utilizada para separar varas em grupos e investigar o perfil de produtividade das varas de cada um dos grupos formados. Por exemplo, ao separar um conjunto de 10 varas em 3 grupos, poderíamos detectar um grupo que está produzindo pouco em relação à quantidade de funcionários, ou então um conjunto formado por uma vara só, que possui processos de execução de títulos extrajudiciais em excesso. Dessa forma, a identificação e investigação das características desses grupos poderiam ajudar em ações estratégicas do tribunal, como critérios para alocação de recursos (investimento em varas problemáticas) e criação de treinamentos (a partir da identificação de varas-modelo).

Ao comparar varas é usual considerar informações de orçamento, recursos humanos (número de funcionários e magistrados), e informações de movimentação processual (número de processos distribuídos, tamanho do acervo, quantidade de julgamentos, etc). Não se pode comparar diretamente maçã com banana: por exemplo, varas especializadas em execução fiscal são difíceis de comparar varas de família.

A base de dados do projeto foi obtida automaticamente através de ferramentas de web scraping e mineração de documentos PDF. Os dados contêm informações de quantidade de funcionários e diversas contagens mensais da vara como acervo, número de sentenças e número de distribuições.

O produto final do projeto foi chamado de "waze do judiciário". 
Trata-se de um aplicativo online de visualização interativa, 
em que o usuário pode selecionar as entrâncias, alguns tipos de varas e
a quantidade de grupos a serem formados.

## Data tidying

Uma base de dados é considerada "tidy" se

- Cada observação é uma linha do bd.
- Cada variável é uma coluna do bd.
- Para cada unidade observacional temos um `data.frame` separado (possivelmente com chaves de associação).

O objetivo em *data wrangling* é extrair e transformar uma base de dados até que ela esteja em formato *tidy*. Em seguida, mostraremos como fizemos isso no exemplo das câmaras. Adicionalmente, vamos apresentar como foram trabalhados os arquivos PDF no caso do Waze do judiciário.

## Web scraping

Este documento contém algumas melhores práticas na contrução de ferramentas no R
que baixam e processam informações de sites disponíveis na web. 
O objetivo é ajudar o desenvolvedor a produzir um pacote que seja fácil 
de adaptar no tempo.

O documento foi construído com base na experiência em web scrapers simples, 
contruídos para acessar listas de páginas pré-definidas. Isto é muito 
diferente de web crawlers não supervisionados, como o do Google, que 
vão passeando pelas páginas de forma indefinida. Por conta disso, o nome 
crawler poderia até ser um pouco inadequado, mas estamos mantendo por falta de
um nome melhor para definir essa tarefa.

Também é importante mencionar que só estamos trabalhando com páginas que
são acessíveis publicamente. Caso tenha interesse e "raspar" páginas que
precisam de autenticação, recomendamos que estude os termos de uso do site.

Para ilustrar este texto, usaremos como exemplo o código utilizado no trabalho das câmaras, 
que acessa o site do Tribunal de Justiça de São Paulo para obter informações de 
processos judiciais. Trabalharemos principalmente com a Consulta de Jurisprudência e a
Consulta de de Processos de Segundo Grau.

## Informações iniciais

Antes de tudo, verifique se existe alguma forma mais fácil de conseguir os
dados que necessita. Construir um web scraper do zero é muitas vezes uma 
tarefa dolorosa e, caso o site seja atualizado, pode ser que boa parte do
trabalho seja inútil. Se os dados precisarem ser extraídos apenas uma vez, 
verifique com o pessoal que mantém o site se eles não podem fazer a extração
que precisa. Se os dados precisarem ser atualizados, verifique se a entidade 
não possui uma API para acesso aos dados.

Ao escrever um web scraper, as primeiras coisas que devemos pensar são

- Como o site a ser acessado foi contruído, se tem limites de requisições,
utilização de cookies, states, etc.
- Como e com que frequência o site é atualizado, tanto em relação à sua
interface como em relação aos dados que queremos extrair.
- Como conseguir a lista das páginas que queremos acessar.
- Qual o caminho percorrido para acessar uma página específica.

Sugerimos como melhores práticas dividir todas as atividades em três tarefas
principais: i) buscar; ii) coletar e iii) processar. Existem casos em que a
etapa de busca é desnecessária (por exemplo, se já sabemos de antemão
quais são as URLs que vamos acessar).

Na maior parte dos casos, deixar os algoritmos de *download* e *parsing* dos dados 
em funções distintas é uma boa prática pois aumenta o 
controle sobre o que as ferramentas estão fazendo, 
facilita o debug e a atualização. Em alguns casos, no entanto, 
isso pode tornar o código mais ineficiente e os 
arquivos obtidos podem ficar pesados.

### Diferença entre procurar, baixar e processar.

Procurar documentos significa, de uma forma geral, utilizar ferramentas de 
busca (ou acessar links de um site) para obter informações de uma nova 
requisição a ser realizada. Ou seja, essa etapa do scraper serve para 
"procurar links" que não sabíamos que existiam previamente. Isso será
resolvido através da função `cjsg`.

Baixar documentos, no entando, significa simplesmente acessar páginas 
pré-estabelecidas e salvá-las em disco. Em algumas situações, os documentos 
baixados (depois de limpos) podem conter uma nova lista de páginas a serem 
baixadas, formando iterações de coletas. A tarefa de baixar documentos
pré-estabelecidos será realizada pela função `cposg`.

Finalmente, processar documentos significa carregar dados acessíveis em disco e 
transformar os dados brutos uma base *tidy*. 
Não existe um limite para a profundidade dessa estruturação
de dados. Geralmente, no entanto, separamos a etapa de estruturação em duas
atividades: i) transformar arquivos não-estruturados em um arquivos 
semi-estruturados (e.g. um arquivo HTML em uma tabela mais um conjunto de
textos livres) e ii) transformar arquivos semi-estruturados em uma base
analítica (estruturada). A tarefa de processar as páginas será realizada pelas 
funções `parse_cjsg` e `parse_cpopg`.

Como veremos no decorrer do documento, no caso do TJSP, teremos um fluxo 
"look for" -> "collect" -> "scrape" -> "collect" -> "scrape" para conseguir
nossos dados.

## Procurar documentos

A tarefa de listar os documentos que queremos obter geralmente pode ser 
realizada de duas formas: i) utilizar uma ferramenta de busca do site e
ii) acessar as páginas a partir do resultado de uma pesquisa. Dependendo
do caso, será necessário realizar:

- Uma busca e uma paginação;
- Uma busca e muitas paginações;
- Muitas buscas e uma paginação por busca;
- Muitas buscas e muitas paginações por busca.

No exemplo de ilustração, nosso caso é de _uma busca e muitas paginações_.
Acesse a página do [e-SAJ](http://esaj.tjsp.jus.br/cjpg/) e clique em
"Consultar" para ter uma ideia de como é essa página. A página (acessada
no dia `r Sys.Date()`) é uma ferramenta de busca com vários campos, que 
permite pesquisa com dados em branco. Na parte de baixo o site mostra uma
série de documentos, organizados em páginas de dez em dez resultados.

Para resolver o problema, precisaremos de duas funções principais, uma que faz
a busca e outra que acessa uma página específica (que será repetida várias 
vezes). Utilizaremos as funções `look_for` e `paginate` para cada um desses
problemas.

### Search docs

```{r}
cjpg_url <- function() {
  u <- 'https://esaj.tjsp.jus.br/cjpg/pesquisar.do'
  u
}
```

A função `search_docs` precisa ser capaz de realizar uma pesquisa e retornar
a resposta do servidoe que contém a primeira página dos resultados. Para 
isso, ela recebe uma lista com dados da busca (do formulário) a url base e um
método para realizar a requisição, podendo ser 'get' ou 'post'. Caso a pesquisa
seja mais complicada, é possível adicionar também uma função que sobrepõe a
busca padrão.

__Futuro__: A função também realizará algumas tarefas conhecidas
de forma automática. Primeiramente acessa a página inicial, verifica se
ela contém certos tipos de tags ocultas, como '__VIEWSTATE' para páginas
em aspx ou 'javax.server.faces' para páginas em java faces e adicionará
esses parâmetros automaticamente na requisição, quando esta for do tipo
'POST'.

No nosso caso, a requisição é um simples 'GET', mas com muitos parâmetros.
Assim, o nosso pacote, dentro do esquema do pacotes `crawlr`, ficaria algo
como:

```{r eval=FALSE}
cjpg_search_data <- list(
  'dadosConsulta.nuProcesso' = '',
  'dadosConsulta.pesquisaLivre' = 'danos morais',
  'dadosConsulta.dtInicio' = '01/10/2014',
  'dadosConsulta.dtFim' = '01/11/2014'
)

cjpg_search_result <- cjpg_search_data %>% 
  search_docs(url = cjpg_url(), method = 'get')

cjpg_search_result
```

Também é possível incluir os parâmetros diretamente na função `search_docs`

```{r eval=FALSE}
cjpg_search_result <- search_docs(
  url = cjpg_url(), 
  'dadosConsulta.nuProcesso' = '',
  'dadosConsulta.pesquisaLivre' = 'danos morais',
  'dadosConsulta.dtInicio' = '01/10/2014',
  'dadosConsulta.dtFim' = '01/11/2014',
  method = 'get'
)
```

No RStudio, é possível visualizar a página baixada com a função `visualize`. O
documento aparecerá na 

```{r eval=FALSE}
visualize(cjpg_search_result)
```

**OBS:** A imagem fica "feia" pois está sem a folha de estilos e as imagens.

Em alguns casos ser uma boa prática criar funções que facilitam a entrada 
de parâmetros de busca. No nosso exemplo, existem parâmetros necessários na
requisição que não precisam ser preenchidos, e parâmetros que precisam ser
preenchidos de uma maneira específica, como as datas, que precisam ser
inseridas no formato '%d/%m/%Y'. Assim, incluimos uma função de "ajuda".

```{r}
cjpg_parms <- function(livre = '', classes = '', assuntos = '',
                        data_inicial = '', data_final = '', varas = '') {
  classes = paste0(classes, collapse = ',')
  assuntos = paste0(assuntos, collapse = ',')
  varas = paste0(varas, collapse = ',')
  if(data_inicial != '' & data_final != '') {
    # aqui eu uso o pacote lubridate para construir a data no formato
    # que o e-SAJ exige.
    cod_data_inicial <- paste(lubridate::day(data_inicial),
                              lubridate::month(data_inicial),
                              lubridate::year(data_inicial) ,
                              sep = '/')
    cod_data_final <- paste(lubridate::day(data_final),
                            lubridate::month(data_final),
                            lubridate::year(data_final) ,
                            sep = '/')
  }
  parms <- list('dadosConsulta.nuProcesso' = '',
                'dadosConsulta.pesquisaLivre' = livre,
                'classeTreeSelection.values' = classes,
                'assuntoTreeSelection.values' = assuntos,
                'varasTreeSelection.values' = varas,
                'dadosConsulta.dtInicio' = cod_data_inicial,
                'dadosConsulta.dtFim' = cod_data_final)
  parms
}
```

Dessa forma, a chamada ficaria um pouco mais padronizada.

```{r eval=FALSE}
cjpg_search_data <- cjpg_parms(livre = 'danos morais', 
                               data_inicial = '2014-10-01', 
                               data_final = '2014-11-01')

# por default method = "get"
cjpg_search_result <-  cjpg_search_data %>% search_docs(cjpg_url())
```

É possível utilizr a função `cjpg_parms` com dados incluídos diretamente na
função através do parâmetro `parm_fun`:

```{r eval=FALSE}
cjpg_search_result <- search_docs(url = cjpg_url(),
                                  livre = 'danos morais', 
                                  data_inicial = '2014-10-01', 
                                  data_final = '2014-11-01',
                                  parm_fun = cjpg_parms)
```

Por fim, é uma boa prática criar uma função que extrai o número de páginas
a serem acessadas pela paginação. Geralmente esse número existe pois as
ferramentas de busca usualmente mostram o número de resultados.

```{r eval=FALSE}
cjpg_npag <- function(r) {
  val <- xml2::read_html(httr::content(r, 'text')) %>%
    xml2::xml_find_all(".//*[@id = 'resultados']//td") %>%
    `[[`(1) %>%
    xml2::xml_text() %>%
    stringr::str_trim() %>%
    stringr::str_match('de ([0-9]+)')
  num <- as.numeric(val[1, 2])
  num
}

cjpg_npag(cjpg_search_result$result)
```

Podemos adicionar essa função como parâmetro `npag_fun` de nossa função 
`search_docs`. 

```{r eval=FALSE}
cjpg_search_result <- search_docs(url = cjpg_url(),
                                  livre = 'danos morais', 
                                  data_inicial = '2014-10-01', 
                                  data_final = '2014-11-01',
                                  parm_fun = cjpg_parms,
                                  npag_fun = cjpg_npag)
```

O objeto retornado pela função `search_docs` é um objeto do tipo
`searchdoc`, que guarda, além da resposta da requisição web, a url base 
utilizada, a lista com os parâmetros, e o número de páginas, estes últimos 
somente se os parâmetros `parm_fun` e `npag_fun` forem informados.

### Paginate

Após conseguir os resultados pela ferramenta de busca e acessar os resultados, 
o próximo desafio é baixar as páginas dos resultados. Realizar a paginação nada
mais é do que repetir a tarefa de acessar uma página diversas vezes, mudando
somente o parâmetro da página.

Algumas vezes, é possível que a URL base para acessar a paginação seja
diferente da ferramenta de busca. Esse é o caso do nosso exemplo. Nesses 
casos, podemos criar uma nova função para guardar essa URL.

```{r}
cjpg_url_pag <- function() {
  u <- 'https://esaj.tjsp.jus.br/cjpg/trocarDePagina.do'
  u
}
```

A função `paginate` recebe como parâmetros 

- `.sch`. Um objeto de classe `searchdoc`.
- `pags`. As páginas que serão acessadas. O valor padrão é 'all', 
indicando que todas as páginas devem ser baixadas. Como alternativa, é 
possível informar ou um vetor nomeado `c(from = min, to = max)`, onde `min` 
e `max` são os extremos do intervalo de páginas que se deseja acessar, ou 
ainda um vetor numérico com os índices desejados.
- `pag_parm`, indicando o nome do parâmetro que identifica a página no site.

Alguns parâmetros opcionais a serem incluídos são

- `method` o método para acessar a página, entre 'get' e 'post', caso este seja
diferente do método utilizado na função `search_docs`.
- `url` a url para acessar a página, caso esta seja diferente da utilizada
na função `search_docs`.
- `name_fun` uma função para nomear os arquivos salvos. Mostraremos um exemplo
em seguida.
- `wait` tempo de espera entre cada requisição. Pode ser útil caso o site
tenha algum limitador.
- `path`, o caminho para salvar os arquivos. Por default é o próprio diretório.

Utilização básica de `paginate`

```{r eval=FALSE}
cjpg_search_result %>% 
  paginate(pags = 1:30, parm_pag= 'pagina', 
           url = cjpg_url_pag(), path = 'data-raw/cjpg')
```

```{r eval=FALSE}
dir('data-raw/cjpg')
```

No exemplo, fizemos o download de 30 páginas a partir do resultado da pesquisa.
Os arquivos salvos são arquivos `.rds` que guardam os resultados das 
requisições. Eles devem ser lidos com a função `readRDS`.

Com isso, conseguimos baixar as páginas que **listam** os itens que queremos
baixar. Em muitos casos, esse   

__Futuro:__ Infelizmente, alguns sites não permitem a inclusão do número de
uma página como parâmetro para realizar a paginação. Muitas vezes esses sites
possuem somente um botão "Next". No futuro vamos adaptar a função `paginate`
para esses casos.

**Onde guardar os dados?** Ao construir um pacote que utiliza o pacote 
`crawlr`, pode fazer sentido guardar os dados baixados dentro do próprio
pacote, para reprodutibilidade. Nesse caso, o melhor lugar para guardar esses
dados é na pasta `data-raw`, como sugerido por Hadley Wickham no livro
[r-pkgs](http://r-pkgs.had.co.nz). No entanto, Se os dados forem muito 
volumosos, colocá-los dentro do pacote pode ser ruim para colocar no GitHub e 
no CRAN. Por isso, pode ser necessário colocar esses documentos numa pasta
externa ao pacote. Para garantir a reprodutibilidade, recomendo que criem um
pacote no R cujo objetivo é guardar somente esses dados, e coloque esse pacote
em um repositório na nuvem (Dropbox, por exemplo). No pacote que contém as
funções de extração, guarde os dados já processados (se couberem) num arquivo
`.rda` dentro da pasta `data` do pacote.

## Processar

## TODO

- Mais exemplos.
- Melhor documentação.
- Adicionar métodos para usar selenium.

## Manipulação de dados com dplyr

A manipulação de dados é uma tarefa usualmente dolorosa e demorada, 
podendo tomar a maior parte do tempo da análise. 
No entanto,
como nosso interesse geralmente é na modelagem dos dados, essa tarefa é muitas vezes negligenciada.

O `dplyr` é um dos pacotes mais úteis para realizar manipulação de dados, e procura aliar 
simplicidade e eficiência de uma forma bastante elegante. Os scripts em `R` que fazem uso 
inteligente dos verbos `dplyr` e as facilidades do operador _pipe_ tendem a ficar mais legíveis e 
organizados, sem perder velocidade de execução.

> "(...) The fact that data science exists as a field is a colossal failure of statistics. To me, [what I do] is what statistics is all about. It is gaining insight from data using modelling and visualization. Data munging and manipulation is hard and statistics has just said that’s not our domain."

> Hadley Wickham

Por ser um pacote que se propõe a realizar um dos trabalhos mais árduos da análise estatística,
e por atingir esse objetivo de forma elegante, eficaz e eficiente, o `dplyr` pode ser considerado 
como uma revolução no `R`.

### Trabalhando com tibbles

```{r}
pnud_muni <- tbl_df(pnud_muni)
pnud_muni
```

### Filosofia do Hadley para análise de dados

<img src="http://r4ds.had.co.nz/diagrams/data-science-wrangle.png" style="width: 400px;"/>

### As cinco funções principais do dplyr

- `filter`
- `mutate`
- `select`
- `arrange`
- `summarise`

### Características

- O _input_  é sempre um `data.frame` (`tbl`), e o _output_  é sempre um `data.frame` (`tbl`).
- No primeiro argumento colocamos o `data.frame`, e nos outros argumentos colocamo o que queremos fazer.
- A utilização é facilitada com o emprego do operador `%>%`

### Vantagens

- Utiliza `C` e `C++` por trás da maioria das funções, o que geralmente torna o código mais eficiente.
- Pode trabalhar com diferentes fontes de dados, como bases relacionais (SQL) e `data.table`.

### select

- Utilizar `starts_with(x)`, `contains(x)`, `matches(x)`, `one_of(x)`, etc.
- Possível colocar nomes, índices, e intervalos de variáveis com `:`.

```{r}
# por indice (nao recomendavel!)
pnud_muni %>%
  select(1:10)
```

```{r}
# especificando nomes (maneira mais usual)
pnud_muni %>%
  select(ano, ufn, municipio, idhm)
```

```{r}
# intervalos e funcoes auxiliares (para economizar trabalho)
pnud_muni %>%
  select(ano:municipio, starts_with('idhm'))
```

### filter

- Parecido com `subset`.
- Condições separadas por vírgulas é o mesmo que separar por `&`.

```{r}
# somente estado de SP, com IDH municipal maior que 80% no ano 2010
pnud_muni %>%
  select(ano, ufn, municipio, idhm) %>%
  filter(ufn=='São Paulo', idhm > .8, ano == 2010)
```

```{r}
# mesma coisa que o anterior
pnud_muni %>%
  select(ano, ufn, municipio, idhm) %>%
  filter(ufn=='São Paulo' & idhm > .8 & ano == 2010)
```

```{r}
# !is.na(x)
pnud_muni %>%
  select(ano, ufn, municipio, idhm, pea) %>%
  filter(!is.na(pea))
```

```{r}
# %in%
pnud_muni %>%
  select(ano, ufn, municipio, idhm) %>%
  filter(municipio %in% c('CAMPINAS', 'SÃO PAULO'))
```

### mutate

- Parecido com `transform`, mas aceita várias novas colunas iterativamente.
- Novas variáveis devem ter o mesmo `length` que o `nrow` do bd oridinal ou `1`.

```{r}
pnud_muni %>%
  select(ano, ufn, municipio, idhm) %>%
  filter(ano == 2010) %>%
  mutate(idhm_porc = idhm * 100,
         idhm_porc_txt = paste(idhm_porc, '%'))
```

```{r}
# media de idhm_l e idhm_e
pnud_muni %>%
  select(ano, ufn, municipio, starts_with('idhm')) %>%
  filter(ano == 2010) %>%
  mutate(idhm2 = (idhm_e + idhm_l) / 2)
```

```{r, eval = FALSE}
# errado
pnud_muni %>%
  select(ano, ufn, municipio, starts_with('idhm')) %>%
  filter(ano == 2010) %>%
  mutate(idhm2 = mean(c(idhm_e, idhm_l)))

# uma alternativa (+ demorada)
pnud_muni %>%
  select(ano, ufn, municipio, starts_with('idhm')) %>%
  filter(ano == 2010) %>%
  rowwise() %>%
  mutate(idhm2 = mean(c(idhm_e, idhm_l)))
```

### arrange

- Simplesmente ordena de acordo com as opções.
- Utilizar `desc` para ordem decrescente.

```{r}
pnud_muni %>%
  select(ano, ufn, municipio, idhm) %>%
  filter(ano == 2010) %>%
  mutate(idhm_porc = idhm * 100,
         idhm_porc_txt = paste(idhm_porc, '%')) %>%
  arrange(idhm)
```

```{r}
pnud_muni %>%
  select(ano, ufn, municipio, idhm) %>%
  filter(ano == 2010) %>%
  mutate(idhm_porc = idhm * 100,
         idhm_porc_txt = paste(idhm_porc, '%')) %>%
  arrange(desc(idhm))
```

### summarise

- Retorna um vetor de tamanho `1` a partir de uma conta com as variáveis.
- Geralmente é utilizado em conjunto com `group_by`.
- Algumas funções importantes: `n()`, `n_distinct()`.

```{r}
pnud_muni %>%
  filter(ano == 2010) %>%  
  group_by(ufn) %>%
  summarise(n = n(), 
            idhm_medio = mean(idhm),
            populacao_total = sum(popt)) %>%
  arrange(desc(idhm_medio))
```

```{r}
pnud_muni %>%
  filter(ano == 2010) %>%  
  count(ufn)
```

```{r}
pnud_muni %>%
  group_by(ano, ufn) %>%
  tally() %>%
  head # nao precisa de parenteses!
```

### Data Tidying com tidyr

### spread

- "Joga" uma variável nas colunas

```{r}
pnud_muni %>%
  group_by(ano, ufn) %>%
  summarise(populacao = sum(popt)) %>%
  ungroup() %>%
  spread(ano, populacao)
```

### gather

- "Empilha" o banco de dados

```{r}
pnud_muni %>%
  filter(ano == 2010) %>%
  select(ufn, municipio, starts_with('idhm_')) %>%
  gather(tipo_idh, idh, starts_with('idhm_'))
```

### Funções auxiliares

- `unite` junta duas ou mais colunas usando algum separador (`_`, por exemplo).
- `separate` faz o inverso de `unite`, e uma coluna em várias usando um separador.

### Um pouco mais de manipulação de dados

- Para juntar tabelas, usar `inner_join`, `left_join`, `anti_join`, etc.
- Para realizar operações mais gerais, usar `do`.
- Para retirar duplicatas, utilizar `distinct`.

### Acessando páginas web 

### Armazenamento de documentos web

### Trabalhando com arquivos pdf

## Text mining

### Expressões regulares

### Trabalhando com datas

